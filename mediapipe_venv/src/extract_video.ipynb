{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGR_MP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP3rU9jxbIca"
      },
      "source": [
        "Video Gesture Recognition with MediaPipe\n",
        "---CSCE 5280\n",
        "---Team members:\n",
        "-------Solomon Ubani ( solomonubani@my.unt.edu ) \n",
        "       Sulav Poudyal ( sulav697@gmail.com ) \n",
        "       Yen Pham ( yenpham@my.unt.edu ) \n",
        "       Khoa Ho ( khoaho@my.unt.edu ) \n",
        "       Stephanie Brooks( StephanieBrooks2@my.unt.edu )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_bVSztgwSAw",
        "outputId": "8c6d7e9a-2d23-4cd3-9769-ddebe7f6e228"
      },
      "source": [
        "!pip install mediapipe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.8.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 30.4 MB 78 kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (0.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.19.5)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.15.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from mediapipe) (0.37.0)\n",
            "Requirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.3.2)\n",
            "Installing collected packages: mediapipe\n",
            "Successfully installed mediapipe-0.8.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2HrDvPOwUvc"
      },
      "source": [
        "# Computer Vision features\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import np_utils\n",
        "import mediapipe as mp\n",
        "\n",
        "# Data processing\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# File path & temporal processes\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Modeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN_LDHSzwWS6"
      },
      "source": [
        "# Set the holistic model\n",
        "mp_holistic = mp.solutions.holistic\n",
        "\n",
        "# Set the drawing utilities\n",
        "mp_drawing = mp.solutions.drawing_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeCvf9yxwYPT"
      },
      "source": [
        "def mediapipe_detection(image, model):\n",
        "    \n",
        "    # Converts the image color.\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
        "    \n",
        "    # Prevents image write\n",
        "    image.flags.writeable = False                  \n",
        "    \n",
        "    # Makes the prediction.\n",
        "    results = model.process(image)\n",
        "    \n",
        "    # Enables image write.\n",
        "    image.flags.writeable = True\n",
        "    \n",
        "    # Convert back to BGR.\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) \n",
        "    \n",
        "    # Return the image and prediction results.\n",
        "    return image,results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAHEjlMJwZvh"
      },
      "source": [
        "# draw_landmarks: Takes a frame and results then applies the landmark \n",
        "# visualizations to hand and pose.\n",
        "def draw_landmarks(image, results):\n",
        "    \n",
        "    # Draw left hand points.                    \n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, \n",
        "                              mp_holistic.HAND_CONNECTIONS)\n",
        "    # Draw right hand points.\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, \n",
        "                              mp_holistic.HAND_CONNECTIONS) \n",
        "    \n",
        "    # Draw pose points.\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, \n",
        "                              mp_holistic.POSE_CONNECTIONS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgVxumHbwdnX"
      },
      "source": [
        "# extract_keypoints: Gets the x,y,z coordinates of the keypoints of a frame and returns a concatenated\n",
        "# array of those coordinates for the pose, left, and right hand.\n",
        "def extract_keypoints(results):\n",
        "\n",
        "    # Gets and flattens the coordinates for each of the landmark areas. If there\n",
        "    # are no values for the frame, 0's are returned.\n",
        "    pose = np.array([[res.x, res.y, res.z, res.visibility] for \n",
        "                     res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
        "    lh = np.array([[res.x, res.y, res.z] for \n",
        "                   res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "    rh = np.array([[res.x, res.y, res.z] for \n",
        "                   res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
        "    \n",
        "    # Returns the concatenated np array for each of the landmarks.\n",
        "    return np.concatenate([pose, lh, rh])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF3Sdxx7wfbL"
      },
      "source": [
        "# extractKPFromVid: Performs keypoints extraction on each frame of the input video\n",
        "# and saves the keypoints to a numpy array folder\n",
        "def extractKPFromVid(videoMP4,action):\n",
        "  with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "    # START\n",
        "    print(\"START KP extraction from vid: \\n\")\n",
        "    # Sets count.\n",
        "    count = 0\n",
        "    \n",
        "    # Set the numpyList\n",
        "    numpyList = []\n",
        "\n",
        "    # Sets the videoFile name. **INCLUDE** the extension\n",
        "    videoFile = videoMP4\n",
        "\n",
        "    # Captures the video\n",
        "    cap = cv2.VideoCapture(videoMP4)\n",
        "\n",
        "    # Set the framerate \n",
        "    frameRate = cap.get(5)\n",
        "\n",
        "    print(\"framerate: \",frameRate,\"\\n\")\n",
        "\n",
        "    # While the video is running, read in the video frames\n",
        "    # and extract the keypoints to a file. \n",
        "    while(cap.isOpened()):\n",
        "      print(\"Start video processing. . .\")\n",
        "      # Sets the frame number\n",
        "      frameId = cap.get(1)\n",
        "\n",
        "      # Reads in the frame.\n",
        "      ret, frame = cap.read()\n",
        "      print(ret)\n",
        "      # Display the video frame with landmarks overlaid.\n",
        "      #cv2_imshow(frame)\n",
        "\n",
        "      # If there are no more frames, the capturing stops.\n",
        "      # Otherwise, the next frame is read in.\n",
        "      if (ret != True):\n",
        "        print(\"End of video: \",videoMP4)\n",
        "        break\n",
        "\n",
        "      # ---- Extract and append the keypoints----.\n",
        "      # Keypoints detections.\n",
        "      image,results = mediapipe_detection(frame,holistic)\n",
        "\n",
        "      # Increment count\n",
        "      count+=1\n",
        "\n",
        "      #---Export keypoints---\n",
        "      # Get the keypoints array\n",
        "      keypoints = extract_keypoints(results)\n",
        "\n",
        "      # Append the keypoints to the numpyList.\n",
        "      if(count<89):\n",
        "        numpyList.append(keypoints)     \n",
        "        break\n",
        "\n",
        "    # Stops the capture.\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows\n",
        "\n",
        "    # Output finish message.\n",
        "    print (\"Frame Capture Finished.\")\n",
        "\n",
        "    # Return the list of numpy arrays.\n",
        "    return numpyList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ass9p7_hwr3J"
      },
      "source": [
        "# Set the folder path for the numpy arrays.\n",
        "DATA_PATH = os.path.join('/content/Data')\n",
        "\n",
        "# Read in the text file to a dataframe.\n",
        "#df_gestures = pd.read_csv(\"/content/Data/labelmap.txt\",sep=\",\",header=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er8im5BW2Ckx"
      },
      "source": [
        "Video keypoint Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcNChuJOQ8Eg"
      },
      "source": [
        "# Set the gestures, window, and sequences.\n",
        "gestures = ['Down','Left','Right','Up']\n",
        "window,sequences = [],[]\n",
        " \n",
        "# Creates the map for gesture to classification values.\n",
        "gestureLabelMap = {label:num for num,label in enumerate(gestures)}\n",
        "\n",
        "# Loops through each video per gesture, processes the video keypoints,\n",
        "#  and builds a list of concatenated numpy arrays.\n",
        "for gesture in gestures:\n",
        "\n",
        "  for root,dirs,files in os.walk(os.path.join(DATA_PATH,gesture)):\n",
        "    print(os.path.join(DATA_PATH,gesture))\n",
        "    for fil in files:\n",
        "      # Extract frames and set the list of numpy arrays from the video.\n",
        "      window = extractKPFromVid(os.path.join(DATA_PATH,gesture,fil),gesture)\n",
        "\n",
        "  # Append the numpyLists for the gesture.\n",
        "  sequences.append(window)\n",
        "  print(\"Sequence complete for: \",gesture,\"\\nSEQUENCE: \",sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCN4HTPB7NxW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cef1b0d4-1eb8-479f-e1db-2c60bac9131f"
      },
      "source": [
        "np.array(labels).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epCe_EVTPs-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca21e9f-650e-4f2a-f6b6-e804fc99c442"
      },
      "source": [
        "# Set X to list of numpy arrays.\n",
        "X = np.array(sequences)\n",
        "\n",
        "# Output X dimensions.\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 30, 258)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSUalwF7Gg8c"
      },
      "source": [
        "# Set the y value\n",
        "y=to_categorical(labels).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8RNao6BTsa1"
      },
      "source": [
        "Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAkpnu87Tnxt"
      },
      "source": [
        "# Set the testing/training data.\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5ZFyhzNURr_"
      },
      "source": [
        "Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "154P4WSYf7FZ"
      },
      "source": [
        "# Training tracking\n",
        "log_dir = os.path.join('Logs')\n",
        "tb_callback = TensorBoard(log_dir=log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGfeA3jXUQ_B"
      },
      "source": [
        "# Set the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the LSTM layers. \n",
        "# **Note: input_shape: If X.shape values were [a,b,c] it would just be b,c.\n",
        "# If X.shape outputs (4,30,258), the input_shape is (30,258)**\n",
        "model.add(LSTM(64,return_sequences=True,activation='relu',input_shape=(30,258)))\n",
        "model.add(LSTM(128,return_sequences=True,activation='relu'))\n",
        "model.add(LSTM(64,return_sequences=False,activation='relu'))\n",
        "# Add the Dense layers\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dense(gestures.shape[0],activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2zfRzy8b7qn"
      },
      "source": [
        "# Compile the model\n",
        "# multiclass classification model --> categorical cross entropy used.\n",
        "model.compile(optimizer='Adam',loss='categorical_crossentropy', metrics = ['categorical_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNhrY2R3fjdb"
      },
      "source": [
        "# Fit the model\n",
        "model.fit(X_train,y_train,epochs=1000,callbacks=[tb_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7kTK9Xsga4O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
