{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGR_MP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_bVSztgwSAw",
        "outputId": "51abd1dd-8e99-4b15-aec0-b91c0a1e0742"
      },
      "source": [
        "!pip install mediapipe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.7/dist-packages (0.8.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.19.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (0.12.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.2.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n",
            "Requirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from mediapipe) (0.37.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.15.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.1.2.30)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.4.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2HrDvPOwUvc"
      },
      "source": [
        "\n",
        "# Computer Vision features\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import np_utils\n",
        "import mediapipe as mp\n",
        "\n",
        "# Data processing\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# File path & temporal processes\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN_LDHSzwWS6"
      },
      "source": [
        "# Set the holistic model\n",
        "mp_holistic = mp.solutions.holistic\n",
        "\n",
        "# Set the drawing utilities\n",
        "mp_drawing = mp.solutions.drawing_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeCvf9yxwYPT"
      },
      "source": [
        "def mediapipe_detection(image, model):\n",
        "    \n",
        "    # Converts the image color.\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
        "    \n",
        "    # Prevents image write\n",
        "    image.flags.writeable = False                  \n",
        "    \n",
        "    # Makes the prediction.\n",
        "    results = model.process(image)\n",
        "    \n",
        "    # Enables image write.\n",
        "    image.flags.writeable = True\n",
        "    \n",
        "    # Convert back to BGR.\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) \n",
        "    \n",
        "    # Return the image and prediction results.\n",
        "    return image,results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAHEjlMJwZvh"
      },
      "source": [
        "# draw_landmarks: Takes a frame and results then applies the landmark visualizations to\n",
        "# hand and pose.\n",
        "def draw_landmarks(image, results):\n",
        "    \n",
        "    # Draw left hand points.                    \n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, \n",
        "                              mp_holistic.HAND_CONNECTIONS)\n",
        "    # Draw right hand points.\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, \n",
        "                              mp_holistic.HAND_CONNECTIONS) \n",
        "    \n",
        "    # Draw pose points.\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, \n",
        "                              mp_holistic.POSE_CONNECTIONS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgVxumHbwdnX"
      },
      "source": [
        "# extract_keypoints: Gets the x,y,z coordinates of the keypoints of a frame and returns a concatenated\n",
        "# array of those coordinates for the pose, left, and right hand.\n",
        "def extract_keypoints(results):\n",
        "\n",
        "    # Gets and flattens the coordinates for each of the landmark areas. If there\n",
        "    # are no values for the frame, 0's are returned.\n",
        "    pose = np.array([[res.x, res.y, res.z, res.visibility] for \n",
        "                     res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
        "    lh = np.array([[res.x, res.y, res.z] for \n",
        "                   res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "    rh = np.array([[res.x, res.y, res.z] for \n",
        "                   res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
        "    \n",
        "    # Returns the concatenated np array for each of the landmarks.\n",
        "    return np.concatenate([pose, lh, rh])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KFdxnB0LAOj"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ass9p7_hwr3J"
      },
      "source": [
        "# Set the folder path for the numpy arrays.\n",
        "DATA_PATH = os.path.join('Data')\n",
        "\n",
        "# Get the names of the gestures from the dataset dataframe.\n",
        "# ********EXTRACT THE GESTURE NAME FROM THE DATASET HERE*******\n",
        "# gestures = np.array(getGestureNameList(dataFrame, columnNumber))\n",
        "gestures = np.array(['thumbsup'])\n",
        "\n",
        "# Set the number of videos contained in the dataset.\n",
        "# ********EXTRACT THE NUMBER OF ROWS FROM THE DATASET HERE*******\n",
        "no_sequences = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6im8DGLm1FeS"
      },
      "source": [
        "Create folders for each of the gestures. ****THIS NEEDS WORK****"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK_2i1Fsw39o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "d3e22da1-e332-462a-b46f-563ee131cf57"
      },
      "source": [
        "# Loops through each of the gestures to make a folder for the main action\n",
        "# and subfolders for each video of the action.\n",
        "for gesture in gestures:\n",
        "  dirmax = np.max(np.array(os.listdir(os.path.join(DATA_PATH, gesture))).astype(int))\n",
        "\n",
        "  # Make a subfolder for each video within the gesture. If the folder already\n",
        "  # exists, pass.\n",
        "  for sequence in range(1,no_sequences+1):\n",
        "    try:\n",
        "      os.makedirs(os.path.join(DATA_PATH,action,str(dirmax+sequence)))\n",
        "    except:\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-039a444ce229>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# and subfolders for each video of the action.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgesture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgestures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mdirmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgesture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# Make a subfolder for each video within the gesture. If the folder already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/thumbsup'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVPcNUmFpdjK"
      },
      "source": [
        "Takes the video, performs keypoints extractions by frame, and saves the resulting numpy arrays to a folder. **** The numpy_array_path needs some tweaking****"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF3Sdxx7wfbL"
      },
      "source": [
        "# extractFrames: Performs keypoints extraction on each frame of the input video\n",
        "# and saves the keypoints to a numpy array folder\n",
        "def extractFrames(videoMP4,action):\n",
        "  with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "    \n",
        "    # Sets count.\n",
        "    count = 0\n",
        "    \n",
        "    # Sets the videoFile name. **INCLUDE** the extension\n",
        "    videoFile = videoMP4\n",
        "\n",
        "    # Captures the video\n",
        "    cap = cv2.VideoCapture(videoMP4)\n",
        "\n",
        "    # Set the framerate \n",
        "    frameRate = cap.get(5)\n",
        "\n",
        "    # While the video is running, read in the video frames\n",
        "    # and write them to a file. \n",
        "    while(cap.isOpened()):\n",
        "\n",
        "      # Sets the frame number\n",
        "      frameId = cap.get(1)\n",
        "\n",
        "      # Reads in the frame.\n",
        "      ret, frame = cap.read()\n",
        "\n",
        "      # Display the video frame with landmarks overlaid.\n",
        "      #cv2_imshow(frame)\n",
        "\n",
        "      # If there are no more frames, the capturing stops.\n",
        "      # Otherwise, the next frame is read in as a jpg \n",
        "      # to the file.\n",
        "      if (ret != True):\n",
        "          break\n",
        "\n",
        "      # If the frame number is divisible by 4, extract\n",
        "      # and write the keypoints to a subfolder.\n",
        "      if (frameId % 4 == 0):\n",
        "\n",
        "        # Keypoints detections.\n",
        "        image,results = mediapipe_detection(frame,holistic)\n",
        "\n",
        "        #filename =\"frame%d.jpg\" % count;\n",
        "        count+=1\n",
        "        #cv2.imwrite(filename, frame)\n",
        "        #print(filename)\n",
        "\n",
        "        #---Export keypoints---\n",
        "        # Get the keypoints array\n",
        "        keypoints = extract_keypoints(results)\n",
        "\n",
        "        #***** Set the numpy array path(Data/Gesture/GestureVideo#/numpyfile)\n",
        "        numpy_array_path = os.path.join(\"Data\",action,str(count))\n",
        "\n",
        "        # Save the keypoints to the path.\n",
        "        np.save(numpy_array_path,keypoints)\n",
        "\n",
        "    # Stops the capture.\n",
        "    cap.release()\n",
        "    print (\"Frame Capture Finished.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er8im5BW2Ckx"
      },
      "source": [
        "Video keypoint Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSWjiBJNwkqF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "647472bc-e6a9-4d33-eff6-08964c194074"
      },
      "source": [
        "videoGestures = ['thumbsup01.mp4','thumbsup02.mp4','thumbsup03.mp4',\n",
        "             'thumbsup04.mp4']\n",
        "\n",
        "#for vid in videoGestures:\n",
        "#extractFrames('vid')\n",
        "\n",
        "extractFrames('thumbsup01.mp4','thumbsup')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-769d8d8dce45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#extractFrames('vid')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mextractFrames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'thumbsup01.mp4'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'thumbsup'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-9c513af79907>\u001b[0m in \u001b[0;36mextractFrames\u001b[0;34m(videoMP4, action)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Save the keypoints to the path.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_array_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeypoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Stops the capture.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mfile_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/thumbsup/1.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AX8-hM62uij"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}